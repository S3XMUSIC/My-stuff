# Precipitation Data Analysis and Forecasting 

This repository contains a project focused on the analysis of precipitation data and forecasting weather conditions for several years ahead.  
The work was completed as part of the **Data-Engineering-Dataset** course.  
The project is written in **Python**.

---

## üìä Project Overview
The main objective of this project is to:
- Analyze historical precipitation data  
- Perform preprocessing and transformation of raw data  
- Visualize precipitation patterns  
- Build models for **forecasting weather conditions** over multiple years  

This project demonstrates fundamental data engineering and machine learning practices.

---

## üìÇ Dataset
The dataset used in this project can be accessed here:  
[üìé Precipitation dataset (Google Drive)](https://drive.google.com/file/d/1NPjKJoVKQWytdYYEIFn7WQGVL6Tljo_L/view?usp=drive_link)

The dataset is automatically downloaded and processed by the data loader script.

---

## üöÄ Installation & Usage
[![Typing SVG](https://readme-typing-svg.herokuapp.com?color=%2336BCF7&lines=Installation+&+Usage)](https://git.io/typing-svg)

Follow these steps to set up the environment and run the project:

### 1. Create and activate a conda environment
```bash
conda create -n my_env python=3.13 pip
conda activate my_env
```

### 2. Install Poetry
```bash
pip install poetry
```

### 3. Create a new Poetry project (if needed)
```bash
poetry new my_project
cd my_project
```

### 4. Add dependencies with Poetry
```bash
pip install pandas gdown jupyterlab matplotlib seaborn plotly statsmodels numpy
```

### 5. Install dependencies
```bash
poetry install --no-root
```

### 6. Run the data loader script
```bash
python data_loader.py
```

## üîÑ Data Processing Features

The `data_loader.py` script performs the following operations:

- **Automatic Download**: Downloads the dataset from Google Drive using the file ID
- **Data Type Conversion**: 
  - Converts date columns (`Date`, `Date1`) to datetime format
  - Converts all numeric columns to float type for consistent processing
- **Data Validation**: Displays the first 10 rows and data types before/after processing
- **CSV Export**: Saves the processed dataset back to CSV format

## üì∑ Script Output Example

When running the script `data_loader.py`, the dataset is automatically downloaded from Google Drive, saved locally, and processed with data type conversions.

Example run:

```bash
(my_env) C:\Users\cdolg\my_project> python data_loader.py
```

<img width="1107" height="632" alt="image" src="https://github.com/user-attachments/assets/15fd6e94-4398-42c6-a4fc-9c668683588e" />


The script will output:
- First 10 rows of the dataset
- Data types before processing
- Data types after processing
- Confirmation that the dataset has been updated

---

## üõ†Ô∏è Dependencies

Main dependencies required for this project:
- `pandas` - Data manipulation and analysis
- `gdown` - Download files from Google Drive
- `jupyterlab` - Interactive development environment
- `matplotlib` - Data visualization
- `seaborn` - Statistical data visualization
- `statsmodels` - Statistical models and time series decomposition
- `plotly` - Interactive visualizations and dashboards

plotly - Interactive visualizations and dashboards
---

# Precipitation Data Analysis and Forecasting 

This repository contains a project focused on the analysis of precipitation data and forecasting weather conditions for several years ahead.  
The work was completed as part of the **Data-Engineering-Dataset** course.  
The project is written in **Python**.

---

## üìÅ Project Structure

---

```

my_project/
‚îÇ
‚îú‚îÄ‚îÄ etl/                     # –ü–∞–∫–µ—Ç ETL
‚îÇ   ‚îú‚îÄ‚îÄ **init**.py
‚îÇ   ‚îú‚îÄ‚îÄ extract.py            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –±–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è CSV –∏–∑ Google Drive
‚îÇ   ‚îú‚îÄ‚îÄ transform.py          # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤, –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îú‚îÄ‚îÄ load.py               # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ PostgreSQL –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ parquet
‚îÇ   ‚îú‚îÄ‚îÄ validate.py           # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îî‚îÄ‚îÄ main.py               # CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ ETL
‚îÇ
‚îú‚îÄ‚îÄ data/                     # –ü–∞–ø–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ (–ù–ï –ø—É—à–∏—Ç—Å—è –≤ git)
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # –°—ã—Ä—ã–µ CSV-—Ñ–∞–π–ª—ã
‚îÇ   ‚îî‚îÄ‚îÄ processed/            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–µ parquet-—Ñ–∞–π–ª—ã
‚îÇ
‚îú‚îÄ‚îÄ creds.db                  # SQLite —Å —É—á–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è PostgreSQL (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .gitignore

````

> ‚ö†Ô∏è –í–∞–∂–Ω–æ: –ø–∞–ø–∫–∞ `data/` –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ `.gitignore` –∏ **–Ω–µ –¥–æ–ª–∂–Ω–∞ –ø–æ–ø–∞–¥–∞—Ç—å –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π**.

---

## üìñ –§—É–Ω–∫—Ü–∏–∏ –ø–∞–∫–µ—Ç–∞

### 1. Extract (`extract.py`)
- –°–∫–∞—á–∏–≤–∞–µ—Ç CSV-—Ñ–∞–π–ª –∏–∑ Google Drive (`file_id` –∏–ª–∏ `url`) —á–µ—Ä–µ–∑ `gdown`.
- –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ CSV, –Ω–∞–ª–∏—á–∏—è —Å—Ç—Ä–æ–∫.
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π CSV –≤ `data/raw/`.

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```bash
python -m etl.main extract --file-id <GDRIVE_FILE_ID> --output dataset.csv
````

---

### 2. Transform (`transform.py`)

* –ß—Ç–µ–Ω–∏–µ CSV –∏–∑ `data/raw/`.
* –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤: –¥–∞—Ç—ã ‚Üí datetime, —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ‚Üí float/int.
* –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.
* –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π CSV –≤ `data/raw/`.

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**

```bash
python -m etl.main transform --input data/raw/dataset.csv --output dataset_cleaned.csv
```

---

### 3. Validate (`validate.py`)

* –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:

  * –Ω–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
  * –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
  * –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–µ –ø—É—Å—Ç–æ–π
* –í—ã–∑–æ–≤ –≤—Å—Ç—Ä–æ–µ–Ω –≤ –ø–∞–π–ø–ª–∞–π–Ω –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π.

---

### 4. Load (`load.py`)

* –ß—Ç–µ–Ω–∏–µ `creds.db` (SQLite) –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö PostgreSQL.
* –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ 100 —Å—Ç—Ä–æ–∫ –≤ –±–∞–∑—É `homeworks`, —Å—Ö–µ–º–∞ `public`.
* –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ parquet: `data/processed/<table_name>_processed.parquet`.
* –ï—Å–ª–∏ Postgres –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç parquet –ª–æ–∫–∞–ª—å–Ω–æ.

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**

```bash
python -m etl.main load --input data/raw/dataset_cleaned.csv --table dolgikh --max-rows 100 --creds-db creds.db
```

---

### 5. Main CLI (`main.py`)

* –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω: **extract ‚Üí transform ‚Üí validate ‚Üí load**.
* –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç: –∫–æ–º–∞–Ω–¥–∞ `run`, `extract`, `transform` –∏–ª–∏ `load`.
* –î–ª—è –∫–æ–º–∞–Ω–¥—ã `run` –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å `--file-id`.

**–ü—Ä–∏–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞:**

```bash
python -m etl.main run --file-id 1NPjKJoVKQWytdYYEIFn7WQGVL6Tljo_L
```

**–ü—Ä–∏–º–µ—Ä —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏:**

```bash
python -m etl.main run \
    --file-id 1NPjKJoVKQWytdYYEIFn7WQGVL6Tljo_L \
    --output dataset.csv \
    --cleaned-name dataset_cleaned.csv \
    --table dolgikh \
    --max-rows 100 \
    --creds-db creds.db
```

---

## üí° –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

* Python >= 3.9
* –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏:

```bash
pip install -r requirements.txt
```

–°–ø–∏—Å–æ–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ `requirements.txt`:

```
pandas
gdown
sqlalchemy
psycopg2-binary
pyarrow
```

> ‚ö†Ô∏è –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ `.parquet` –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω—É–∂–µ–Ω `pyarrow` –∏–ª–∏ `fastparquet`.

---

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞

1. –ü–æ–º–µ—Å—Ç–∏—Ç–µ `creds.db` –≤ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ PostgreSQL).
   –¢–∞–±–ª–∏—Ü–∞ `access` –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: `url`, `port`, `user`, `pass`.
2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∫–∞—Ç–∞–ª–æ–≥–∏ `data/raw/` –∏ `data/processed/` —Å—É—â–µ—Å—Ç–≤—É—é—Ç (—Å–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞—Å—Ç –∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ).
3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ `.gitignore`:

```
/data/
__pycache__/
*.pyc
*.pyo
venv/
env/
.my_env/
.vscode/
.idea/
```

---

## üìå –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã

1. –°–∫–∞—á–∞–π—Ç–µ CSV —á–µ—Ä–µ–∑ extract.
2. –ü—Ä–∏–≤–µ–¥–∏—Ç–µ —Ç–∏–ø—ã —á–µ—Ä–µ–∑ transform.
3. –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
4. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤ PostgreSQL –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ parquet —á–µ—Ä–µ–∑ load –∏–ª–∏ run.

**–ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞ —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**

```
[main] START full run
[extract] CSV –ø—Ä–æ—á–∏—Ç–∞–Ω: shape=(3902, 23)
[transform] –°–æ—Ö—Ä–∞–Ω—ë–Ω –æ—á–∏—â–µ–Ω–Ω—ã–π CSV -> data/raw/dataset_cleaned.csv
[validate] –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–π–¥–µ–Ω–∞: shape=(3902, 23)
[load] –°–æ—Ö—Ä–∞–Ω—ë–Ω parquet -> data/processed/dolgikh_processed.parquet
[load] –¢–∞–±–ª–∏—Ü–∞ 'dolgikh' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ —Å—Ö–µ–º—É public.
[main] FULL RUN complete.
```

---

## ‚úÖ –ó–∞–º–µ—á–∞–Ω–∏—è

* –¢–∞–±–ª–∏—Ü–∞ –≤ PostgreSQL —Å–æ–∑–¥–∞—ë—Ç—Å—è —Å –∏–º–µ–Ω–µ–º –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ `--table` (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é `dolgikh`).
* –ï—Å–ª–∏ `creds.db` –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ parquet.
* –°—Ç–∞—Ä—ã–µ —Å–∫—Ä–∏–ø—Ç—ã `data_loader.py` –∏ `write_to_db.py` **–Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–¥–∞–ª–µ–Ω—ã**.

```

---






